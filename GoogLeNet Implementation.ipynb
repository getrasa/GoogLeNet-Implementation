{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GoogLeNet Implementation\n",
    "In this notebook I'll reimplement famous CNN called GoogLeNet from [this](https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Szegedy_Going_Deeper_With_2015_CVPR_paper.pdf) research paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 224, 224, 3)\n",
      "(10, 1000)\n"
     ]
    }
   ],
   "source": [
    "# This is the section where you can load images and preprocess them.\n",
    "# GoogLeNet input shape is (224, 224, 3)\n",
    "# In my case I'll create toy data\n",
    "train_data = np.random.rand(10, 224, 224, 3)\n",
    "train_labels = np.zeros([10, 1000])\n",
    "for i in range(10):\n",
    "    train_labels[i][random.randint(0, 999)] = 1\n",
    "    \n",
    "print(train_data.shape)\n",
    "print(train_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helping Funcions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_placeholders(img_shape, num_classes):\n",
    "\n",
    "    # Initialize placeholder for inputs x\n",
    "    x = tf.placeholder(tf.float32, [None,] + list(img_shape), name='x')\n",
    "    \n",
    "    # Initialize placeholder for labels y\n",
    "    y = tf.placeholder(tf.float32, [None, num_classes], name='y')\n",
    "    \n",
    "    # Initialize placeholder for dropout\n",
    "    keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "    \n",
    "    return x, y, keep_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Convolution(x_tensor, output_dims, k_size, stride, padding=\"VALID\"):\n",
    "    \n",
    "    # Get the number of channels\n",
    "    x_channels = x_tensor.get_shape().as_list()[-1]\n",
    "    \n",
    "    # Initialize weights/filters and biases\n",
    "    W = tf.Variable(tf.truncated_normal([k_size[0], k_size[1], x_channels, output_dims], stddev=0.01))\n",
    "    b = tf.Variable(tf.truncated_normal([output_dims], stddev=0.01))\n",
    "    \n",
    "    # Perform convolution\n",
    "    conv = tf.nn.conv2d(x_tensor, W, (1, stride[0], stride[1], 1), padding)\n",
    "    conv = tf.nn.bias_add(conv, b)\n",
    "    \n",
    "    # Relu activation function\n",
    "    conv = tf.nn.relu(conv)\n",
    "    \n",
    "    return conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MaxPooling(x_tensor, k_size=(1,1), stride=(1,1), padding=\"VALID\"):\n",
    "    # Reshape into dimensions acceptable by Tensorflow\n",
    "    filter_size = (1, k_size[0], k_size[1], 1)\n",
    "    strides = (1, stride[0], stride[1], 1)\n",
    "    \n",
    "    # Perform max pooling\n",
    "    pool = tf.nn.max_pool(x_tensor, filter_size, strides, padding)\n",
    "    \n",
    "    return pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AvgPooling(x_tensor, k_size=(1,1), stride=(1,1), padding=\"VALID\"):\n",
    "    # Reshape into dimensions acceptable by Tensorflow\n",
    "    filter_size = (1, k_size[0], k_size[1], 1)\n",
    "    strides = (1, stride[0], stride[1], 1)\n",
    "    \n",
    "    # Perform max pooling\n",
    "    pool = tf.nn.avg_pool(x_tensor, filter_size, strides, padding)\n",
    "    \n",
    "    return pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Flatten(x_tensor):\n",
    "    # Get tensor's shape\n",
    "    shape = x_tensor.get_shape().as_list()\n",
    "    \n",
    "    # Get reshaped size\n",
    "    reshaped_size = shape[1] * shape[2] * shape[3]\n",
    "    \n",
    "    # Flatten the matrix\n",
    "    flat = tf.reshape(x_tensor, [-1, reshaped_size])\n",
    "    \n",
    "    return flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FullyConnected(x_tensor, num_output, activation=\"relu\"):\n",
    "    # Get the tensor's shape\n",
    "    shape = x_tensor.get_shape().as_list()\n",
    "    \n",
    "    # Initialize weights and biases\n",
    "    W = tf.Variable(tf.truncated_normal([shape[-1], num_output], stddev=0.01))\n",
    "    b = tf.Variable(tf.truncated_normal([num_output], stddev=0.01))\n",
    "    \n",
    "    # Perform fully connected forward pass\n",
    "    fc = tf.add(tf.matmul(x_tensor, W), b)\n",
    "    \n",
    "    # Activation\n",
    "    if activation == \"relu\":\n",
    "        fc = tf.nn.relu(fc)\n",
    "        \n",
    "    return fc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LocalResponseNormalization(x_tensor):\n",
    "    return tf.nn.local_response_normalization(x_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Dropout(x_tensor, keep_prob):\n",
    "    return tf.nn.dropout(x_tensor, keep_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Inception(x_tensor, dims):\n",
    "    \"\"\"\n",
    "    dims = an array of output dimensions (conv_1x1, conv_3x3_reduced, conv_3x3, conv_5x5_reduced, conv_5x5, pool_reduction_dims)\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1x1 Convolution\n",
    "    conv_1x1 = Convolution(x_tensor, dims[0], k_size=(1,1), stride=(1,1), padding=\"SAME\")\n",
    "    \n",
    "    # 3x3 Convolution\n",
    "    conv_3x3_reduced = Convolution(x_tensor, dims[1], k_size=(1,1), stride=(1,1), padding=\"SAME\")\n",
    "    conv_3x3 = Convolution(conv_3x3_reduced, dims[2], k_size=(3,3), stride=(1,1), padding=\"SAME\")\n",
    "    \n",
    "    # 5x5 Convolution\n",
    "    conv_5x5_reduced = Convolution(x_tensor, dims[3], k_size=(1,1), stride=(1,1), padding=\"SAME\")\n",
    "    conv_5x5 = Convolution(conv_5x5_reduced, dims[4], k_size=(5,5), stride=(1,1), padding=\"SAME\")\n",
    "    \n",
    "    # Pooling\n",
    "    pool = MaxPooling(x_tensor, k_size=(3,3), stride=(1,1), padding=\"SAME\")\n",
    "    pool = Convolution(pool, dims[5], k_size=(1,1), stride=(1,1), padding=\"SAME\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Concatenate the layers depth-wise where axis(0=num examples, 1=height, 2=width, 3=depth)\n",
    "    concat = tf.concat([conv_1x1, conv_3x3, conv_5x5, pool], axis=3)\n",
    "    \n",
    "    return concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(images, labels, batch_size):\n",
    "    ''' Create a generator of image batches as a tuple (inputs, targets) '''\n",
    "    \n",
    "    n_batches = len(images) //batch_size\n",
    "    \n",
    "    # only full batches\n",
    "    images = images[:n_batches*batch_size]\n",
    "    \n",
    "    for idx in range(0, len(images), batch_size):\n",
    "        batch_inputs = images[idx:idx+batch_size]\n",
    "        batch_labels = labels[idx:idx+batch_size]\n",
    "        \n",
    "        yield (np.array(batch_inputs) , np.array(batch_labels, ndmin=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a network\n",
    "According to google's research paper this network consists of 22 layers when counting only layers with parameters(27 if we also count pooling). The overall number of layers (independent building blocks) used for the construction of the network is about 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GoogLeNet(X, num_classes, keep_prob):\n",
    "    \n",
    "    # First Block\n",
    "    model = Convolution(X, 64, k_size=(7,7), stride=(2,2), padding=\"SAME\")\n",
    "    model = MaxPooling(model, k_size=(3,3), stride=(2,2), padding=\"SAME\")\n",
    "    model = LocalResponseNormalization(model)\n",
    "    \n",
    "    # Second Block\n",
    "    model = Convolution(model, 64, k_size=(1,1), stride=(1,1), padding=\"SAME\")\n",
    "    model = Convolution(model, 192, k_size=(3,3), stride=(1,1), padding=\"SAME\")\n",
    "    model = MaxPooling(model, k_size=(3,3), stride=(2,2), padding=\"SAME\")\n",
    "    model = LocalResponseNormalization(model)\n",
    "    \n",
    "    ### INCEPTION LAYERS ###\n",
    "    model = Inception(model, dims = (64, 96, 128, 16, 32, 32))\n",
    "    model = Inception(model, dims = (128,128, 192, 32, 96, 64))\n",
    "    \n",
    "    # Maxpooling\n",
    "    model = MaxPooling(model, k_size=(3,3), stride=(2,2), padding=\"SAME\")\n",
    "    \n",
    "    # Inception Block\n",
    "    model = Inception(model, dims = (192, 96, 208, 16, 48, 64))\n",
    "    model = Inception(model, dims = (160, 112, 224, 24, 64, 64))\n",
    "    model = Inception(model, dims = (128, 128, 256, 24, 64, 64))\n",
    "    model = Inception(model, dims = (112, 144, 288, 32, 64, 64))\n",
    "    model = Inception(model, dims = (256, 160, 320, 32, 128, 128))\n",
    "    \n",
    "    # Maxpooling\n",
    "    model = MaxPooling(model, k_size=(3,3), stride=(2,2), padding=\"SAME\")\n",
    "    \n",
    "    # Inception Block\n",
    "    model = Inception(model, dims = (256, 160, 320, 32, 128, 128))\n",
    "    model = Inception(model, dims = (384, 192, 384, 48, 128, 128))\n",
    "    \n",
    "    \n",
    "    ### OUTPUT ###\n",
    "    model = AvgPooling(model, k_size=(7,7), stride=(1,1), padding=\"VALID\")\n",
    "    model = Dropout(model, keep_prob)\n",
    "    \n",
    "    # Fully Connected Convolution\n",
    "    fc = Convolution(model, num_classes, k_size=(1,1), stride=(1,1), padding=\"SAME\")\n",
    "    \n",
    "    # Softmax activation function for choosing classes\n",
    "    output = tf.nn.softmax(fc)\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 60\n",
    "batch_size = 10\n",
    "learning_rate = 0.01\n",
    "momentum_rate = 0.9\n",
    "keep_probobability = 1\n",
    "num_classes = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph\n",
    "\n",
    "x, y, keep_prob = initialize_placeholders((224, 224, 3), num_classes)\n",
    "out = GoogLeNet(x, num_classes, keep_prob)\n",
    "\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=out, labels=y))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate = learning_rate).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.907762\n",
      "6.907762\n",
      "6.907762\n",
      "6.907762\n",
      "6.907762\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-225-96d96b83ba62>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m             \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcost\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeep_prob\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mkeep_probobability\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\getra\\appdata\\local\\continuum\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    903\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    904\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 905\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    906\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    907\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\getra\\appdata\\local\\continuum\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1138\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1139\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1140\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1141\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1142\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\getra\\appdata\\local\\continuum\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1319\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1320\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1321\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1322\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1323\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\getra\\appdata\\local\\continuum\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1325\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1326\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1327\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1328\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\getra\\appdata\\local\\continuum\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1310\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1311\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1312\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1313\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1314\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\getra\\appdata\\local\\continuum\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1418\u001b[0m         return tf_session.TF_Run(\n\u001b[0;32m   1419\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1420\u001b[1;33m             status, run_metadata)\n\u001b[0m\u001b[0;32m   1421\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1422\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        batches = get_batches(train_data, train_labels, batch_size)\n",
    "        \n",
    "        for i, (inputs, labels) in enumerate(batches):\n",
    "            loss, _ = sess.run([cost, optimizer], feed_dict={x: inputs, y: labels, keep_prob: keep_probobability})\n",
    "            print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
